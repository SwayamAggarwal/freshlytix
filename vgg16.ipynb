{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6ada766-8e90-41a5-8606-36dd63040e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import torchvision\n",
    "from torchvision import transforms, models, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import functional\n",
    "from torchmetrics import Recall, Precision\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98a89dfc-b0a3-4d0a-85da-feee92cc7460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the nutrition data (CSV file)\n",
    "nutrition_df = pd.read_csv(r\"D:\\FRUITSANDVEGETABLESDATASET\\fruitsandvegetablescropped - Copy1.csv\")\n",
    "# Serialize the nutrition data\n",
    "nutrition_data = nutrition_df.to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2660bdc-7b7e-4394-b43b-ea8b45babc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and transformations\n",
    "directory = (r\"D:\\FRUITSANDVEGETABLESDATASET\\Fruit And Vegetable Diseases Dataset - Copy\")\n",
    "transformer = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "data = ImageFolder(root=directory, transform=transformer)\n",
    "class_names = data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ea38bc1-d19e-425a-bbad-e66aed83aef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, validation, and test datasets\n",
    "generator1 = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset, test_dataset = random_split(data, [0.7, 0.2, 0.1], generator=generator1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51cc98f4-6234-418a-b7fb-036bac7c3e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91c9eb0f-60d1-468d-b1ca-319655679f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\AISC LABS\\ANACONDA\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "E:\\AISC LABS\\ANACONDA\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Define VGG-16 Model\n",
    "model = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14d3df53-b864-410d-8ba0-d81e18d0bc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the classifier to match the number of classes\n",
    "num_classes = len(class_names)\n",
    "model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "515d7701-6a5e-4846-a820-c2d466e0039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers except the final fully connected layer\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.classifier.parameters():\n",
    "    if param.shape[0] == num_classes:  # Only unfreeze the last layer\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8685e8f7-e94a-45e4-9caa-8eeea167909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1fd3bda-3e3d-4fcd-8a37-ddcea9934a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6e5e24a-31b1-43fa-bcf3-21451c51bf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\AISC LABS\\ANACONDA\\Lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.7657394759449963\n",
      "Validation Loss: 0.38210254154429446, Validation Accuracy: 88.28951860703312\n",
      "Test Loss: 0.37923633128040185, Test Accuracy: 88.25537726186411\n",
      "Epoch [2/20], Loss: 0.41677801557431543\n",
      "Validation Loss: 0.28747432761856073, Validation Accuracy: 91.27688630932059\n",
      "Test Loss: 0.3084361200959867, Test Accuracy: 90.47456469784909\n",
      "Epoch [3/20], Loss: 0.3259483407263887\n",
      "Validation Loss: 0.26778603877430673, Validation Accuracy: 91.68658245134858\n",
      "Test Loss: 0.2592625808851469, Test Accuracy: 91.87435984977809\n",
      "Epoch [4/20], Loss: 0.27641788214955815\n",
      "Validation Loss: 0.2052011837658145, Validation Accuracy: 93.47900307272107\n",
      "Test Loss: 0.20417963686581378, Test Accuracy: 93.64970979856606\n",
      "Epoch [5/20], Loss: 0.23142455027194608\n",
      "Validation Loss: 0.21166472708202433, Validation Accuracy: 93.39364970979857\n",
      "Test Loss: 0.1983204746287336, Test Accuracy: 93.58142710822807\n",
      "Epoch [6/20], Loss: 0.2048854758833497\n",
      "Validation Loss: 0.20203114555589094, Validation Accuracy: 94.19597132127006\n",
      "Test Loss: 0.1998716216555255, Test Accuracy: 93.82041652441106\n",
      "Epoch [7/20], Loss: 0.17912184763287503\n",
      "Validation Loss: 0.2106884706819973, Validation Accuracy: 93.61556845339706\n",
      "Test Loss: 0.21376957843566072, Test Accuracy: 93.61556845339706\n",
      "Epoch [8/20], Loss: 0.15733789134240406\n",
      "Validation Loss: 0.17639545679910168, Validation Accuracy: 94.45203141003755\n",
      "Test Loss: 0.1739679644714387, Test Accuracy: 94.81051553431205\n",
      "Epoch [9/20], Loss: 0.14059943714855253\n",
      "Validation Loss: 0.17746899653902107, Validation Accuracy: 94.67395015363606\n",
      "Test Loss: 0.1754892440251791, Test Accuracy: 94.77637418914306\n",
      "Epoch [10/20], Loss: 0.13581232052045\n",
      "Validation Loss: 0.1715188547869737, Validation Accuracy: 94.79344486172755\n",
      "Test Loss: 0.1622972370883951, Test Accuracy: 95.39091840218505\n",
      "Epoch [11/20], Loss: 0.12743084534813306\n",
      "Validation Loss: 0.16309606619795625, Validation Accuracy: 95.37384772960054\n",
      "Test Loss: 0.15890962463505354, Test Accuracy: 95.39091840218505\n",
      "Epoch [12/20], Loss: 0.11774459859180197\n",
      "Validation Loss: 0.15842714764858845, Validation Accuracy: 95.30556503926255\n",
      "Test Loss: 0.15628626112416183, Test Accuracy: 95.39091840218505\n",
      "Epoch [13/20], Loss: 0.10774967364275868\n",
      "Validation Loss: 0.1583008209180927, Validation Accuracy: 95.69819050870605\n",
      "Test Loss: 0.14856979028665948, Test Accuracy: 95.66404916353704\n",
      "Epoch [14/20], Loss: 0.09627421085418443\n",
      "Validation Loss: 0.15850430198403215, Validation Accuracy: 95.40798907476955\n",
      "Test Loss: 0.15837357851479855, Test Accuracy: 95.76647319904404\n",
      "Epoch [15/20], Loss: 0.09455581863845047\n",
      "Validation Loss: 0.1509401609168159, Validation Accuracy: 95.88596790713554\n",
      "Test Loss: 0.17665888028422277, Test Accuracy: 95.83475588938204\n",
      "Epoch [16/20], Loss: 0.08635873819952389\n",
      "Validation Loss: 0.1575276829890811, Validation Accuracy: 95.59576647319905\n",
      "Test Loss: 0.14836037290176493, Test Accuracy: 96.14202799590304\n",
      "Epoch [17/20], Loss: 0.08422015292213564\n",
      "Validation Loss: 0.16684244663942854, Validation Accuracy: 95.64697849095255\n",
      "Test Loss: 0.17525133762612075, Test Accuracy: 95.59576647319905\n",
      "Epoch [18/20], Loss: 0.08217852249508255\n",
      "Validation Loss: 0.18566448320100168, Validation Accuracy: 95.01536360532604\n",
      "Test Loss: 0.16776419467463632, Test Accuracy: 95.28849436667805\n",
      "Epoch [19/20], Loss: 0.07278756593773032\n",
      "Validation Loss: 0.15895018519337864, Validation Accuracy: 95.66404916353704\n",
      "Test Loss: 0.16415759848559258, Test Accuracy: 95.76647319904404\n",
      "Epoch [20/20], Loss: 0.07209937970115256\n",
      "Validation Loss: 0.17254614671479523, Validation Accuracy: 95.73233185387504\n",
      "Test Loss: 0.17151511691450655, Test Accuracy: 95.39091840218505\n",
      "Model saved to vgg16_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 20  # Set number of epochs to 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f'Validation Loss: {val_loss/len(val_loader)}, Validation Accuracy: {100 * val_correct / val_total}')\n",
    "\n",
    "    # Test phase\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f'Test Loss: {test_loss/len(test_loader)}, Test Accuracy: {100 * test_correct / test_total}')\n",
    "\n",
    "# Save the trained model along with the nutrition data\n",
    "model_save_path = \"vgg16_model.pth\" \n",
    "model_state = {\n",
    "    'state_dict': model.state_dict(),\n",
    "    'nutrition_data': nutrition_data,\n",
    "    'class_names': class_names\n",
    "}\n",
    "\n",
    "torch.save(model_state, model_save_path)\n",
    "print(f'Model saved to {model_save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66f0d55-4399-4085-abb1-709292b62cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
